{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyannote.core import Segment, Timeline, Annotation\n",
    "from pyannote.metrics.identification import *\n",
    "from pyannote.metrics.detection import *\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation range issue\n",
    "This next cell was intended to test whether generating multiple `Annotation()` objects at the outset (before populating _any_ of them) would provide a workaround for the issue of the visualizations scaling to the largest segment value in memory. (It does not.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_CoraMommy = Annotation()\n",
    "hyp_IsaiahAli = Annotation()\n",
    "hyp_IsaiahJumanji = Annotation()\n",
    "hyp_PreMathterpieces = Annotation()\n",
    "hyp_Mathterpieces = Annotation()\n",
    "hyp_TeacherAri = Annotation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoraMommy\n",
    "This is a ~16 minute storytime session between an adult female (Anna Long-Slade) and a 27-month-old female (Cora). Anna was reading from a series of storybooks and modifying her voice to align with the characters'. Two characters were male, so Anna dropped her voice accordingly. There were segments of continuous back-and-forth between Cora and Anna. Occasionally an adult male (Tim Slade) made comments or reacted to goings-on, but these were rare.\n",
    "\n",
    "The recording was made in the early morning when all other members of the household were asleep. It was captured with a microphone positioned within ~1 meter of both participants. The space was a breakfast nook adjacent to a kitchen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyp_CoraMommy = Annotation()\n",
    "file1 = open('../output_voice_type_classifier/slade_MommyCoraEmotions_16m34s/all.rttm', 'r')\n",
    "for line in file1:\n",
    "    parts = line.strip().split(\" \")\n",
    "#     if parts[7] in [\"SPEECH\", \"KCHI\", \"CHI\", \"FEM\", \"MAL\"]:\n",
    "    if parts[7] in [\"KCHI\", \"FEM\", \"MAL\", \"SPEECH\"]:\n",
    "        hyp_CoraMommy[Segment(pd.to_numeric(parts[3])*1000, (pd.to_numeric(parts[3])+pd.to_numeric(parts[4]))*1000)] = parts[7]\n",
    "hyp_CoraMommy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(\"/home/leemoore/slade_MommyCoraEmotions_16m34s.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IsaiahAli\n",
    "This is a ~5 min. conversation between an adult male (Tim Slade) and a 7-y.o. child (Isaiah) about the child's dog.\n",
    "\n",
    "The recording was captured on the side of a busy suburban road. The microphone was held in Tim's hand at roughly chest height, which was roughly head height for Isaiah. Isaiah was generally within 2-4 meters of the microphone; Tim was within roughly 0.5 meters of the microphone at all time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyp_IsaiahAli = Annotation()\n",
    "file2 = open('../output_voice_type_classifier/slade_Streetside01IsaiahReAli/all.rttm', 'r')\n",
    "for line in file2:\n",
    "    parts = line.strip().split(\" \")\n",
    "    if parts[7] in [\"KCHI\", \"FEM\", \"MAL\"]:\n",
    "        hyp_IsaiahAli[Segment(pd.to_numeric(parts[3])*1000, (pd.to_numeric(parts[3])+pd.to_numeric(parts[4]))*1000)] = parts[7]\n",
    "hyp_IsaiahAli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(\"/home/tslade/slade_Streetside01IsaiahReAli.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IsaiahJumanji\n",
    "This is a ~5 min. conversation between an adult male (Tim Slade) and a 7-y.o. child (Isaiah) about a movie the child had seen.\n",
    "\n",
    "The recording was captured partially on the side of a busy suburban road and partially within a quiet housing development. The microphone was held in Tim's hand at roughly chest height, which was roughly head height for Isaiah. Isaiah was generally within 2-4 meters of the microphone; Tim was within roughly 0.5 meters of the microphone at all time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyp_IsaiahJumanji = Annotation()\n",
    "file3 = open('../output_voice_type_classifier/slade_Streetside02IsaiahReJumanji/all.rttm', 'r')\n",
    "for line in file3:\n",
    "    parts = line.strip().split(\" \")\n",
    "    if parts[7] in [\"KCHI\", \"FEM\", \"MAL\"]:\n",
    "        hyp_IsaiahJumanji[Segment(pd.to_numeric(parts[3])*1000, (pd.to_numeric(parts[3])+pd.to_numeric(parts[4]))*1000)] = parts[7]\n",
    "hyp_IsaiahJumanji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio(\"/home/tslade/slade_Streetside02IsaiahReJumanji.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Mathterpieces\n",
    "This is a ~45 second session between an adult female (Anna Long-Slade) and several female children aged 10 yrs, 8 yrs, 8 yrs, and 27 months. Anna was asking them to gather around for a reading from a math lesson book requiring the participating children to respond to prompts and generate answers to a puzzle. \n",
    "\n",
    "The recording was made in the late morning with a mixer operating in the adjacent room and low levels of noise from the outside. It was captured with a microphone positioned within ~3 meters of all participants. The space was a living room with predominantly hard flooring and walls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyp_PreMathterpieces = Annotation()\n",
    "file4 = open('../output_voice_type_classifier/slade_LivingRoom01Pre-Mathterpieces/all.rttm', 'r')\n",
    "for line in file4:\n",
    "    parts = line.strip().split(\" \")\n",
    "    if parts[7] in [\"KCHI\", \"FEM\", \"MAL\"]:\n",
    "        hyp_PreMathterpieces[Segment(pd.to_numeric(parts[3])*1000, (pd.to_numeric(parts[3])+pd.to_numeric(parts[4]))*1000)] = parts[7]\n",
    "hyp_PreMathterpieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(\"/home/tslade/slade_LivingRoom01Pre-Mathterpieces.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathterpieces\n",
    "This is a ~5 min session between an adult female (Anna Long-Slade) and several female children aged 10 yrs, 8 yrs, 8 yrs, and 27 months. Anna was reading from a math lesson book requiring the participating children to respond to prompts and generate answers to a puzzle. \n",
    "\n",
    "The recording was made in the late morning with a mixer operating in the adjacent room and low levels of noise from the outside. It was captured with a microphone positioned within ~3 meters of all participants. The space was a living room with predominantly hard flooring and walls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyp_Mathterpieces = Annotation()\n",
    "file5 = open('../output_voice_type_classifier/slade_LivingRoom02Mathterpieces/all.rttm', 'r')\n",
    "for line in file5:\n",
    "    parts = line.strip().split(\" \")\n",
    "    if parts[7] in [\"KCHI\", \"FEM\", \"MAL\", \"CHI\"]:\n",
    "        hyp_Mathterpieces[Segment(pd.to_numeric(parts[3])*1000, (pd.to_numeric(parts[3])+pd.to_numeric(parts[4]))*1000)] = parts[7]\n",
    "hyp_Mathterpieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(\"/home/tslade/slade_LivingRoom02Mathterpieces.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TeacherAri\n",
    "This is a ~2 min session featuring female children aged 10-y.o., 8-y.o., and 27-m.o. A piano is played from approximately 1:05-1:50.\n",
    "\n",
    "The recording was made in the late morning with low levels of noise from the outside. It was captured with a microphone positioned within ~3 meters of all participants. The space was a living room with predominantly hard flooring and walls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyp_TeacherAri = Annotation()\n",
    "file6 = open('../output_voice_type_classifier/slade_LivingRoom03TeacherAri/all.rttm', 'r')\n",
    "for line in file6:\n",
    "    parts = line.strip().split(\" \")\n",
    "    if parts[7] in [\"KCHI\", \"FEM\", \"MAL\"]:\n",
    "        hyp_TeacherAri[Segment(pd.to_numeric(parts[3])*1000, (pd.to_numeric(parts[3])+pd.to_numeric(parts[4]))*1000)] = parts[7]\n",
    "hyp_TeacherAri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(\"/home/tslade/slade_LivingRoom03TeacherAri.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Annotation() objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis2.chart()\n",
    "list(hypothesis2.itertracks())\n",
    "hypothesis2.get_timeline()\n",
    "hypothesis2.subset(['KCHI', 'FEM'])\n",
    "hypothesis2.update()\n",
    "print(hypothesis2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/miniconda3/envs/pyannote/lib/python3.7/site-packages/pyannote/core/annotation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../output_voice_type_classifier/slade_MommyCoraEmotions_16m34s/all.rttm',\n",
    "                 sep=' ',\n",
    "                 names=['task','inputFile','one','start','duration',\n",
    "                        'NA_1','NA_2','class','NA_3', 'NA_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_TeacherAriFromDF = Annotation()\n",
    "df = pd.read_csv('../output_voice_type_classifier/slade_LivingRoom03TeacherAri/all.rttm',\n",
    "                 sep=' ',\n",
    "                 names=['task','inputFile','one','start','duration',\n",
    "                        'NA_1','NA_2','class','NA_3', 'NA_4'])\n",
    "\n",
    "df[['start', 'duration']] = df[['start','duration']].astype(float)*1000\n",
    "df['end'] = df['start'] + df['duration']\n",
    "df = df[['task','inputFile','one','start','duration','end','NA_1','NA_2','class','NA_3','NA_4']]\n",
    "df.head()\n",
    "\n",
    "for i in range(0, len(df[df['start'] < 30000])):\n",
    "    hyp_TeacherAriFromDF[Segment(df['start'].loc[i],df['end'].loc[i])] = df['class'].loc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mommy_cora = pd.read_csv('../output_voice_type_classifier/slade_MommyCoraEmotions_16m34s/all.rttm',\n",
    "                         sep=' ',\n",
    "                        names=['task','inputFile','one','start','duration',\n",
    "                        'NA_1','NA_2','class','NA_3', 'NA_4'])\n",
    "pre_math = pd.read_csv('../output_voice_type_classifier/slade_LivingRoom01Pre-Mathterpieces/all.rttm',\n",
    "                         sep=' ',\n",
    "                        names=['task','inputFile','one','start','duration',\n",
    "                        'NA_1','NA_2','class','NA_3', 'NA_4'])\n",
    "math = pd.read_csv('../output_voice_type_classifier/slade_LivingRoom02Mathterpieces/all.rttm',\n",
    "                         sep=' ',\n",
    "                        names=['task','inputFile','one','start','duration',\n",
    "                        'NA_1','NA_2','class','NA_3', 'NA_4'])\n",
    "re_ali = pd.read_csv('../output_voice_type_classifier/slade_Streetside01IsaiahReAli/all.rttm',\n",
    "                         sep=' ',\n",
    "                        names=['task','inputFile','one','start','duration',\n",
    "                        'NA_1','NA_2','class','NA_3', 'NA_4'])\n",
    "re_jumanji = pd.read_csv('../output_voice_type_classifier/slade_Streetside02IsaiahReJumanji/all.rttm',\n",
    "                         sep=' ',\n",
    "                        names=['task','inputFile','one','start','duration',\n",
    "                        'NA_1','NA_2','class','NA_3', 'NA_4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mommy_cora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Columns should be named START, DUR, and LABEL.\n",
    "+ For LABEL: Relabel KCHI and CHI as CHILD, and relabel FEM and MAL as ADULT.\n",
    "+ Subset CHILD and ADULT labels only.\n",
    "+ Sort by START.\n",
    "+ Add UTTERANCE (basically the index - 1 through X).\n",
    "+ Add DUR_TRANS, which is the positive DUR value for CHILD and the negative DUR value for ADULT (this is how we get the diverging y-axis).\n",
    "+ Add LABEL_NUM, which is 1 for CHILD and -1 for ADULT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rttm_to_speaker_activity(df, outfile=None):\n",
    "    \"\"\" Given an RTTM input file, generate a dataframe structured\n",
    "        to support a visualization of type {} and optionally\n",
    "        {export} to a csv located at {outfile}\n",
    "    \"\"\"\n",
    "    # Check whether an outfile has been defined\n",
    "    if outfile is not None:\n",
    "        export=True\n",
    "    \n",
    "    # Drop the columns we don't care about from a base RTTM\n",
    "    vizframe = copy.deepcopy(df) \\\n",
    "                   .drop(\n",
    "                    columns=['task',\n",
    "                             'inputFile',\n",
    "                             'one',\n",
    "                             'NA_1',\n",
    "                             'NA_2',\n",
    "                             'NA_3',\n",
    "                             'NA_4'])\n",
    "    \n",
    "    # Rename columns for our viz's purposes\n",
    "    vizframe = vizframe.rename(columns={\n",
    "                                'start':'START',\n",
    "                                'duration':'DUR',\n",
    "                                'class':'LABEL'\n",
    "                                })\n",
    "    \n",
    "    # Remap the model classes for this viz's purposes\n",
    "    vizframe['LABEL'] = vizframe['LABEL'].map({\n",
    "                              'KCHI':'CHILD',\n",
    "                              'CHI':'CHILD',\n",
    "                              'FEM':'ADULT',\n",
    "                              'MAL':'ADULT'\n",
    "                              })\n",
    "    \n",
    "    # Filter the dataframe to just the 'clean' (non-'SPEECH') classes\n",
    "    vizframe = vizframe[vizframe['LABEL'].isin(['CHILD', 'ADULT'])]\n",
    "    vizframe['LABEL_NUM'] = vizframe['LABEL'] \\\n",
    "                                     .apply(lambda x: 1 if x=='CHILD'\n",
    "                                                        else (-1 if x=='ADULT' else NaN))\n",
    "    vizframe['DUR_TRANS'] = vizframe['LABEL_NUM'] * vizframe['DUR']    \n",
    "\n",
    "    if export:\n",
    "        vizframe.to_csv(outfile)\n",
    "\n",
    "    return vizframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anna_mc = rttm_to_lollipops(mommy_cora, outfile='./AnnaCoraStoryTimeExport.csv')\n",
    "red_pre_math = rttm_to_lollipops(pre_math, outfile='./PreMathterpieces.csv')\n",
    "red_math = rttm_to_lollipops(math, outfile='./Mathterpieces.csv')\n",
    "red_re_ali = rttm_to_lollipops(re_ali, outfile='./Re_Ali.csv')\n",
    "red_re_jumanji = rttm_to_lollipops(re_jumanji, outfile='./Re_Jumanji.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat Re_Jumanji.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ https://stackoverflow.com/questions/44991438/lambda-including-if-elif-else\n",
    "+ https://stackoverflow.com/questions/48872234/using-apply-in-pandas-lambda-functions-with-multiple-if-statements\n",
    "+ https://docs.quantifiedcode.com/python-anti-patterns/readability/comparison_to_none.html\n",
    "+ https://stackoverflow.com/questions/2710940/python-if-x-is-not-none-or-if-not-x-is-none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
