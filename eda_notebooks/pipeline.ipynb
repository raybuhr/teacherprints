{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "from IPython.display import Audio\n",
    "# import pafy\n",
    "import subprocess as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go get the data\n",
    "The goal of this section is to serve up recordings. Those can be in video format or audio format - it doesn't really matter. The output is simply some manner of audio file we can tackle with `ffmpeg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/ffmpeg\r\n"
     ]
    }
   ],
   "source": [
    "!which ffmpeg\n",
    "ffmpeg = '/usr/local/bin/ffmpeg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the audio data to .wav audio data\n",
    "We need our audio files to be in 16Khz .wav format. This section is about taking the inputs in whatever format they are currently found and standardizing them to .wav."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_wav(label, uri, output_dir):\n",
    "    \"\"\" Given a label, a uniform resource id, and an output directory\n",
    "        relative to the current function,\n",
    "        convert the file from its existing format to\n",
    "        a .wav file at 16000 Khz like we need for our models\n",
    "    \"\"\"\n",
    "    input_filepath = uri\n",
    "    output_filepath = os.path.join(output_dir, label, '.wav')\n",
    "    \n",
    "#     ffmpeg_path = sp.Popen('which ffmpeg', stdout=sp.PIPE, stderr=sp.PIPE)\n",
    "    ffmpeg_path = sp.run(['which', 'ffmpeg'],\n",
    "                         capture_output=True,\n",
    "                         text=True).stdout.replace('\\n','')\n",
    "    audio_codec = 'pcm_s16le'\n",
    "    \n",
    "    \n",
    "    # This is the command we use to convert our mp3s to wav elsewhere\n",
    "    # ffmpeg  -hide_banner\n",
    "    #         -nostats\n",
    "    #         -loglevel fatal\n",
    "    #         -nostdin\n",
    "    #         -i ./ferris-bueller.mp3\n",
    "    #         -acodec pcm_s16le\n",
    "    #         -ac 1\n",
    "    #         -ar 16000\n",
    "    #         -y ./ferris-bueller.wav\n",
    "    \n",
    "    # Define the args as a dict to make them easier to deal with\n",
    "    audio_dl_args = [ffmpeg_path,\n",
    "    #     '-ss', str(ts_start),    # The beginning of the trim window if any\n",
    "    #     '-t', str(duration),     # Specify the duration of the output\n",
    "        '-hide_banner',\n",
    "        '-nostats',\n",
    "        '-loglevel', 'fatal',\n",
    "        '-nostdin',\n",
    "        '-i', input_filepath,    # Specify the input video URL\n",
    "        '-vn',                   # Suppress the video stream\n",
    "        '-ac', '1',              # Set the number of channels\n",
    "        '-acodec', audio_codec,  # Specify the output encoding\n",
    "        '-ar', '16000',          # Specify the audio sample rate\n",
    "        '-y', output_filepath]\n",
    "    \n",
    "    proc = sp.Popen(audio_dl_args, stdout=sp.PIPE, stderr=sp.PIPE)\n",
    "    stdout, stderr = proc.communicate()\n",
    "    if proc.returncode != 0:\n",
    "        print(stderr)\n",
    "    else:\n",
    "        print(f'Exported converted audio to {output_filepath}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the .wav data through the model\n",
    "We need to pipe our .wav files through the predictions models to generate `.rttm` outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the model's prediction outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of our model's predictions is an `.rttm` file. We want to load that into a DataFrame to be able to derive the analyses we need for our visualizations and reporting. There are several steps in this process.\n",
    "1. Get a [pandas] DataFrame from the RTTM\n",
    "1. Transform the DataFrame. This can be either \n",
    "  1. a very simple segment-indexed DataFrame that filters out `SPEECH` segments and only leaves `FEM`, `MAL`, and `CHI` segments, or\n",
    "  1. a time-indexed DataFrame that retains `SPEECH`, `MAL`, `FEM`, `CHI`, etc. labels as one-hot encoded features. (The current version is indexed at the millisecond level.)\n",
    "1. Generate a CSV from the transformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the functions we're going to need for this pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get a pandas DF from the RTTM\n",
    "def df_from_rttm(rttm):\n",
    "    \"\"\" Given an RTTM file, parses it into a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(rttm,\n",
    "                     sep=' ',\n",
    "                     names=['task','inputFile','one','start','duration',\n",
    "                     'NA_1','NA_2','class','NA_3', 'NA_4'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2A: Generate an utterance-indexed pandas DF\n",
    "def rttm_to_utterance_indexed_speaker_activity(df, outfile=None):\n",
    "    \"\"\" Given an RTTM input file, generate a dataframe structured\n",
    "        to support a visualization of type 'Speaker Activity' and optionally\n",
    "        export to a csv located at {outfile}\n",
    "\n",
    "        df = Pandas DataFrame containing a standard .rttm file\n",
    "        outfile = destination for exported CSV (path, filename, extension)\n",
    "    \"\"\"\n",
    "\n",
    "    # Check whether an outfile has been defined\n",
    "    if outfile is not None:\n",
    "        export = True\n",
    "\n",
    "    # Drop the columns we don't care about from a base RTTM\n",
    "    vizframe = copy.deepcopy(df) \\\n",
    "        .drop(\n",
    "        columns=[\n",
    "            'task',\n",
    "            'inputFile',\n",
    "            'one',\n",
    "            'NA_1',\n",
    "            'NA_2',\n",
    "            'NA_3',\n",
    "            'NA_4'])\n",
    "\n",
    "    # Rename columns for our viz's purposes\n",
    "    vizframe = vizframe.rename(columns={\n",
    "        'start': 'START',\n",
    "        'duration': 'DUR',\n",
    "        'class': 'LABEL'\n",
    "    })\n",
    "\n",
    "    # Remap the model classes for this viz's purposes\n",
    "    vizframe['LABEL'] = vizframe['LABEL'].map({\n",
    "        'KCHI': 'CHILD',\n",
    "        'CHI': 'CHILD',\n",
    "        'FEM': 'ADULT',\n",
    "        'MAL': 'ADULT'\n",
    "    })\n",
    "\n",
    "    # Filter the dataframe to just the 'clean' (non-'SPEECH') classes\n",
    "    vizframe = vizframe[vizframe['LABEL'].isin(['CHILD', 'ADULT'])]\n",
    "    vizframe['LABEL_NUM'] = vizframe['LABEL'] \\\n",
    "        .apply(lambda x: 1 if x == 'CHILD'\n",
    "               else (-1 if x == 'ADULT' else NaN))\n",
    "    vizframe['DUR_TRANS'] = vizframe['LABEL_NUM'] * vizframe['DUR']\n",
    "    vizframe['COUNT'] = 1\n",
    "\n",
    "    if export:\n",
    "        vizframe.to_csv(outfile)\n",
    "\n",
    "    return vizframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2B: Generate a time-indexed (curr. millisecond-indexed) DF\n",
    "\n",
    "# We need to know the latest timestamp in a given label-subsetted\n",
    "# DF in order to fill in the appropriate timestamps\n",
    "\n",
    "def get_latest_timestamp_needed(input_df):\n",
    "    \"\"\" Given an RTTM-derived dataframe,\n",
    "        extract the last timestamp we'll need\n",
    "        as a scalar\n",
    "    \"\"\"\n",
    "    last_row = input_df[input_df['start']==input_df['start'].max()][['start', 'duration']]\n",
    "    last_row.reset_index(drop=True, inplace=True)\n",
    "    latest_timestamp = last_row.at[0,'start'] + last_row.at[0,'duration']\n",
    "    return round(latest_timestamp, 1)\n",
    "\n",
    "\n",
    "# We want a time-indexed range. In this case, we're working with a millisecond\n",
    "# level of resolution b/c that's how our data comes out in the RTTM. Collapsing\n",
    "# to a less-granular resolution should be done in a later step.\n",
    "\n",
    "def build_millisecond_range(start, duration, value='', valname='value', verbose=False):\n",
    "    \"\"\" Given a start time, and end time, and a value,\n",
    "        create a dataframe with a timedelta index containing\n",
    "        that value for the range between the endpoints\n",
    "    \"\"\"\n",
    "    \n",
    "    # Turn the endpoints into a millisecond-denominated target\n",
    "    low_end = pd.to_timedelta(round(start, 1), unit='milliseconds')\n",
    "    span = pd.to_timedelta(round(duration, 1), unit='milliseconds')\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Low end: {low_end}\\nDuration: {span}\\n\")\n",
    "    \n",
    "    # Create a range between them\n",
    "    rng = pd.timedelta_range(low_end, low_end+span, freq='L')\n",
    "\n",
    "    # Turn that series into a DataFrame and rename the index for clarity\n",
    "    df = pd.Series(value, index=rng).to_frame(name=valname)\n",
    "    df.index.name='milliseconds'\n",
    "    if verbose:\n",
    "        print(df.head(3))\n",
    "    return df\n",
    "\n",
    "\n",
    "# Given an .rttm-derived DF of the format defined above, we can generate\n",
    "# millisecond-indexed DF with one-hot encoded labels for our classes of interest\n",
    "\n",
    "def build_millisecond_indexed_df(input_df, label_list=[], verbose=True):\n",
    "    \"\"\" Given an RTTM-generated DataFrame, generate a pivoted DF\n",
    "        containing all of the labels of interest, one-hot encoded\n",
    "    \"\"\"\n",
    "    \n",
    "    # To generalize, we're taking in a second-denominated timestamp from our\n",
    "    # RTTM. We need to multiply by 1000 to get our milliseconds\n",
    "    input_df[['start', 'duration']] = input_df[['start', 'duration']]*1000\n",
    "    \n",
    "    max_seconds_needed = get_latest_timestamp_needed(input_df)\n",
    "    if verbose:\n",
    "        print(f'max_seconds_needed is of type {type(max_seconds_needed)} and equal to {max_seconds_needed}')\n",
    "    outer_df = build_millisecond_range(\n",
    "                0,\n",
    "                max_seconds_needed,\n",
    "                np.nan,\n",
    "                'base',\n",
    "                verbose=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'The outer_df frame will contain {len(outer_df)} records.')\n",
    "    \n",
    "    # Loop through labels, subsetting the original DF so\n",
    "    # we can merge it back into the main outer DF\n",
    "    for label in label_list:\n",
    "        print(f'Processing label: {label}\\n')\n",
    "\n",
    "        if label not in input_df['class'].unique():\n",
    "            print(f'Label {label} not found in this dataset')\n",
    "            label_base_df = pd.DataFrame(columns=[label])\n",
    "            continue\n",
    "        \n",
    "        # Gotta avoid errors from accidentally manipulating original DFs\n",
    "        temp_df = copy.deepcopy(input_df)\n",
    "\n",
    "        # Generate a temp_df that contains only records for the label of interest\n",
    "        temp_df = input_df[['start','duration','class']][input_df['class']==label]\n",
    "\n",
    "        if verbose:\n",
    "            print(f'The temp_df subset for label {label} contains {len(temp_df)} rows')\n",
    "            print(temp_df.head(5))\n",
    "        \n",
    "        # The subsetted DF retains the original index unless you reset it\n",
    "        temp_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        if verbose:\n",
    "            print(f'The temp_df frame is as follows:\\n{temp_df}')\n",
    "\n",
    "        # Creating the base DF for this label - ranges from 0 to the earliest record\n",
    "        label_base_df = build_millisecond_range(0, temp_df['start'].min(),\n",
    "                                     np.nan, label, verbose=verbose)\n",
    "        if verbose:\n",
    "            # The head() will always be the same, so we need to look at the tail() to verify\n",
    "            print(f'\\n>>> The last few rows of the label_base_df for label {label} are:\\n{label_base_df.tail()}')\n",
    "            print(f\"\\n>>> Base DF size for label {label}: {len(temp_df)}\\n\")\n",
    "\n",
    "        for i in range(1, len(temp_df)):\n",
    "            label_base_df = label_base_df.append(\n",
    "                build_millisecond_range(\n",
    "                    temp_df.loc[i]['start'],\n",
    "                    temp_df.loc[i]['duration'],\n",
    "                    str(label), str(label),\n",
    "                    verbose=verbose\n",
    "                ))\n",
    "\n",
    "            if verbose:\n",
    "                print(f'\\n>>> Base DF size after {i} rounds: {len(label_base_df)}')\n",
    "                print(f'\\n>>> The head:\\n{label_base_df.head(10)}\\n>>> The tail:\\n{label_base_df.tail(10)}')\n",
    "                print(f'\\n>>> A few of its contents:\\n{label_base_df[~label_base_df[label].isna()].head(5)}')\n",
    "\n",
    "        # Creating a placeholder for the update call\n",
    "        outer_df[label] = np.nan\n",
    "        \n",
    "        # When attempting the update method:\n",
    "        print(f'Attempting update with DF from label {label}')\n",
    "        outer_df.update(label_base_df, overwrite=True)\n",
    "        \n",
    "        if verbose:\n",
    "            try:\n",
    "                print(outer_df[~outer_df[label].isna()].head())\n",
    "            except:\n",
    "                print(outer_df)\n",
    "            \n",
    "        # When attempting the merge method:\n",
    "#         outer_df = pd.merge(left=outer_df, left_index=True,\n",
    "#                             right=label_base_df, right_index=True,\n",
    "#                             how='inner', suffixes=('_base',''),\n",
    "#                             indicator=True, validate='1:1')\n",
    "#         if verbose:\n",
    "#             print(outer_df[~outer_df[f'{label}_y'].isna()].head(5))\n",
    "\n",
    "    return outer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the pipeline\n",
    "Here we actually go ahead and use the functions defined above to compose an actual pipeline of transformation for our files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b''\n",
      "b''\n",
      "b''\n"
     ]
    }
   ],
   "source": [
    "oftf_dict = {\n",
    "    'oftf-aj':'OneFishTwoFish_AnnaJacobson.wav',\n",
    "    'oftf-ts':'OneFishTwoFish_TimothySlade.wav',\n",
    "    'oftf-ds':'OneFishTwoFish_DavidSlade.wav',\n",
    "}\n",
    "for k, v in oftf_dict.items():\n",
    "    convert_to_wav(k, v, './')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 805680\r\n",
      "drwxr-xr-x@ 65 tsslade  staff   2.0K Jun 28 15:10 \u001b[34m.\u001b[m\u001b[m\r\n",
      "drwxr-xr-x@ 12 tsslade  staff   384B Jun 15 17:53 \u001b[34m..\u001b[m\u001b[m\r\n",
      "drwxr-xr-x@ 10 tsslade  staff   320B Jun 19 17:27 \u001b[34m.boneyard\u001b[m\u001b[m\r\n",
      "drwxr-xr-x@ 24 tsslade  staff   768B Jun 27 09:53 \u001b[34m.git\u001b[m\u001b[m\r\n",
      "-rw-r--r--@  1 tsslade  staff    77B Jun 22 12:23 .gitignore\r\n",
      "drwxr-xr-x@  7 tsslade  staff   224B Jun 27 10:07 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m\r\n",
      "drwxr-xr-x@  6 tsslade  staff   192B Jun 15 17:53 \u001b[34m.secrets\u001b[m\u001b[m\r\n",
      "drwxr-xr-x@  2 tsslade  staff    64B May 30 09:48 \u001b[34m.test\u001b[m\u001b[m\r\n",
      "-rw-r--r--@  1 tsslade  staff   688K Jun 14 21:26 AR31_021108a_dampened.mp3\r\n",
      "-rw-r--r--@  1 tsslade  staff   6.7M Jun 14 21:23 AR31_021108a_dampened.wav\r\n",
      "-rw-r--r--@  1 tsslade  staff   2.6K Jun 22 12:23 AudioSetReference.md\r\n",
      "drwxr-xr-x@  5 tsslade  staff   160B Jun 22 12:23 \u001b[34mDiViMe\u001b[m\u001b[m\r\n",
      "-rw-r--r--@  1 tsslade  staff    16M Jun 27 10:03 OneFishTwoFish_AnnaJacobson.wav\r\n",
      "-rw-r--r--@  1 tsslade  staff    20M Jun 27 10:04 OneFishTwoFish_DavidSlade.wav\r\n",
      "-rw-r--r--@  1 tsslade  staff    22M Jun 27 10:05 OneFishTwoFish_TimothySlade.wav\r\n",
      "-rw-r--r--@  1 tsslade  staff    67B May 30 07:03 README.md\r\n",
      "-rw-r--r--@  1 tsslade  staff    18K Jun 25 19:27 SladeFathersDayExperimentation.ipynb\r\n",
      "-rw-r--r--@  1 tsslade  staff   7.7M Jun 22 12:23 YouTubeDL-ProofOfConcept.ipynb\r\n",
      "-rw-r--r--@  1 tsslade  staff   1.3M Jun 14 06:38 allocation_big-cow02-06.csv\r\n",
      "-rw-r--r--@  1 tsslade  staff   119K Jun 14 05:35 audio_dl_progress_2020-06-14.txt\r\n",
      "-rw-r--r--@  1 tsslade  staff   196K Jun 14 14:22 audiosetTargetsDLedBy_2020-06-14_14h11.csv\r\n",
      "-rw-r--r--@  1 tsslade  staff   170M Jun 14 11:10 audiosetdl.log\r\n",
      "-rw-r--r--@  1 tsslade  staff    18M Jun 24 14:54 ava_download_poc.ipynb\r\n",
      "-rw-r--r--@  1 tsslade  staff   276B Jun 22 12:23 base-env.yml\r\n",
      "-rw-r--r--@  1 tsslade  staff   3.0K Jun 22 12:23 devEnvironmentSetup.md\r\n",
      "-rw-r--r--@  1 tsslade  staff    12K Jun 22 12:23 ec2-run-instance.yml\r\n",
      "-rw-r--r--@  1 tsslade  staff    44M Jun 14 18:08 engage-ny-147627759.wav\r\n",
      "-rw-r--r--@  1 tsslade  staff   1.2M Jun 14 19:26 engage-ny-147641946.wav\r\n",
      "-rw-r--r--@  1 tsslade  staff   9.5M Jun 14 20:56 engage-ny-147641946_dampened.wav\r\n",
      "-rw-r--r--@  1 tsslade  staff   2.9M Jun 14 21:00 engage-ny-147643132.wav\r\n",
      "-rw-r--r--@  1 tsslade  staff    27M Jun 14 21:04 engage-ny-147643132_dampened.wav\r\n",
      "-rw-r--r--@  1 tsslade  staff   1.1M Jun 14 05:36 eval_segments.csv.bak\r\n",
      "-rw-r--r--@  1 tsslade  staff   2.7K Jun 22 12:23 get-audioset.sh\r\n",
      "-rw-r--r--@  1 tsslade  staff   2.7K Jun 22 12:23 get-audioset.sh.bak\r\n",
      "-rw-r--r--@  1 tsslade  staff    60K Jun 22 12:23 get-nyc-teaching-vids.py\r\n",
      "-rw-r--r--@  1 tsslade  staff    28M Jun 27 10:28 get_famous_scenes_audio.ipynb\r\n",
      "-rw-r--r--@  1 tsslade  staff   5.0K Jun  4 18:46 gettingStartedAWS (RTI-P345670's conflicted copy 2020-06-08).md\r\n",
      "-rw-r--r--@  1 tsslade  staff    16K Jun 22 12:23 gettingStartedAWS.md\r\n",
      "drwxr-xr-x@  9 tsslade  staff   288B Jun 22 12:23 \u001b[34mlanding_page\u001b[m\u001b[m\r\n",
      "-rw-r--r--@  1 tsslade  staff    74K Jun 22 12:23 logo-teacherprints.jpg\r\n",
      "-rw-r--r--@  1 tsslade  staff   2.2K Jun 22 12:23 make-users.sh\r\n",
      "-rw-r--r--@  1 tsslade  staff   247K Jun 27 10:20 millisecond_index.ipynb\r\n",
      "-rw-r--r--@  1 tsslade  staff   651B Jun 22 12:23 modeling-env.yml\r\n",
      "-rw-r--r--@  1 tsslade  staff   2.7K Jun 22 13:03 ny_1946.rttm\r\n",
      "-rw-r--r--@  1 tsslade  staff   4.2K Jun 22 13:04 ny_3132.rttm\r\n",
      "-rw-r--r--@  1 tsslade  staff   5.4K Jun 22 13:36 ny_7759.csv\r\n",
      "-rw-r--r--@  1 tsslade  staff   8.1K Jun 22 13:04 ny_7759.rttm\r\n",
      "-rw-r--r--@  1 tsslade  staff    17K Jun 28 15:10 pipeline.ipynb\r\n",
      "-rw-r--r--@  1 tsslade  staff   266B Jun 25 19:27 provision-conda-envs.sh\r\n",
      "-rw-r--r--@  1 tsslade  staff   1.8K Jun 22 12:23 provision-miniconda.sh\r\n",
      "-rw-r--r--@  1 tsslade  staff   4.5K Jun 22 12:23 provisioning.sh\r\n",
      "drwxr-xr-x@  3 tsslade  staff    96B Jun 22 12:23 \u001b[34mreference\u001b[m\u001b[m\r\n",
      "-rw-r--r--@  1 tsslade  staff   1.6K May 30 11:05 researchNotes_AudioSet.md\r\n",
      "-rw-r--r--@  1 tsslade  staff   1.9K Jun 22 12:23 rttm_from_interval_set.py\r\n",
      "-rw-r--r--@  1 tsslade  staff   1.9K Jun 24 21:31 rttm_to_speaker_activity.py\r\n",
      "-rw-r--r--@  1 tsslade  staff    13M Jun 24 14:54 slade_FathersDayExperimentation.ipynb\r\n",
      "-rw-r--r--@  1 tsslade  staff   2.9K Jun 22 12:23 step-by-step.sh\r\n",
      "-rw-r--r--@  1 tsslade  staff   4.6M Jun 14 14:57 testing.wav\r\n",
      "-rw-r--r--@  1 tsslade  staff   526B Jun 22 12:23 ts-environment.yml\r\n",
      "-rw-r--r--@  1 tsslade  staff   1.3K Jun 22 12:23 upload-start-files.sh\r\n",
      "drwxr-xr-x@  5 tsslade  staff   160B Jun  7 00:34 \u001b[34mvandam_practice\u001b[m\u001b[m\r\n",
      "-rw-r--r--@  1 tsslade  staff   180B Jun 22 12:23 viz-env.yml\r\n",
      "-rw-r--r--@  1 tsslade  staff   676B Jun 16 09:12 vpc-security-group.yaml\r\n",
      "-rw-r--r--@  1 tsslade  staff   676B Jun 22 12:23 vpc-security-group.yml\r\n",
      "-rw-r--r--@  1 tsslade  staff   6.7K Jun 22 12:23 workspace-setup.sh\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffmpeg = sp.run(['which', 'ffmpeg'], capture_output=True, text=True).stdout.replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffmpeg = ffmpeg.replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/bin/ffmpeg'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ https://docs.python.org/3/library/subprocess.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
